# -*- coding: utf-8 -*-
"""LLM-RAG NLP pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VL3WPoofF2DdxUCdjqJjcm_ncRdJDRFf

# **Feedback Data - Sentiment Analysis**

### Install and Import dependencies
"""

#!pip install --quiet openai pandas tqdm

#pip install openai==0.28

#pip install nlpaug

import re
from collections import Counter
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
import io, pandas as pd, os, random, time
import os, getpass
from typing import List
import pandas as pd

import nlpaug.augmenter.word as naw

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

import pandas as pd, torch, numpy as np
from torch.utils.data import Dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          Trainer, TrainingArguments)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt, seaborn as sns, numpy as np
from scipy.special import softmax
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_curve, auc)

"""#### Load Data"""

#Replace with local feedback.csv file path
file_path = Path("/content/drive/MyDrive/feedback_data.csv")
df = pd.read_csv(file_path)
df.info()

df.head()

"""### Exploratory Data Analysis

#### Duplicate Row
"""

# Duplicate checks
duplicates = df.duplicated(subset="Feedback").sum()
print(f"\nDuplicate feedback rows: {duplicates}")
print(df[df.duplicated(subset='Feedback')])

#drop duplicated row and display df sape

df.drop_duplicates(subset="Feedback", inplace=True)
print(f"DataFrame shape after dropping duplicates: {df.shape}")

"""####Value counts


"""

sentiment_counts = df["Sentiment"].value_counts()
print("\n--- Sentiment distribution ---")
print(sentiment_counts)

"""####Word Frequency"""

CUSTOM_STOPWORDS = {
    "the", "and", "to", "a", "of", "was", "is", "it", "in", "for", "on",
    "with", "that", "this", "as", "at", "an", "be", "are", "but", "so",
    "we", "our", "i", "you", "not", "very", "too", "me"
}

def clean_tokenise(text: str) -> list[str]:
    """Lowercase, strip punctuation, split and drop stop‑words."""
    text = re.sub(r"[^\w\s]", " ", text.lower())
    tokens = text.split()
    return [tok for tok in tokens if tok not in CUSTOM_STOPWORDS]

df["tokens"] = df["Feedback"].apply(clean_tokenise)

def freq(tokens_list: list[list[str]], top_n: int = 20) -> list[tuple[str, int]]:
    all_words = (tok for toks in tokens_list for tok in toks)
    return Counter(all_words).most_common(top_n)

top20 = freq(df["tokens"], top_n=20)
top10_pos = freq(df[df["Sentiment"] == "Positive"]["tokens"], 10)
top10_neg = freq(df[df["Sentiment"] == "Negative"]["tokens"], 10)

"""#### Word cloud"""

sns.set(style="whitegrid")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Sentiment distribution
sns.barplot(
    x=sentiment_counts.index,
    y=sentiment_counts.values,
    ax=axes[0, 0],palette='Set2'
)
axes[0, 0].set_title("Sentiment Distribution")
axes[0, 0].set_ylabel("Count")

# Top‑20 words overall
words, freqs = zip(*top20)
sns.barplot(
    x=list(freqs),
    y=list(words),
    ax=axes[0, 1]
)
# Word cloud – Positive
pos_text = " ".join(tok for tok in df[df["Sentiment"] == "Positive"]["tokens"].explode())
wc_pos = WordCloud(width=400, height=200, background_color="white").generate(pos_text)
axes[1, 0].imshow(wc_pos, interpolation="bilinear")
axes[1, 0].set_title("Word Cloud – Positive")
axes[1, 0].axis("off")

# Word cloud – Negative
neg_text = " ".join(tok for tok in df[df["Sentiment"] == "Negative"]["tokens"].explode())
wc_neg = WordCloud(width=400, height=200, background_color="white").generate(neg_text)
axes[1, 1].imshow(wc_neg, interpolation="bilinear")
axes[1, 1].set_title("Word Cloud – Negative")
axes[1, 1].axis("off")

fig.tight_layout()
plt.show()

"""## Data Augmentation

#### NLPAug
"""

import pandas as pd
import nlpaug.augmenter.word as naw

# Number of augmentations per original sample [trying N_AUG=2,3,5]
N_AUG = 5

# Initializing contextual based augmenter
aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action='substitute', aug_p=0.3)
augmented_rows = []

#class counts could be used to balance augmentation (augment minority class more)
# [Feedback data is fairly balanced so not using double augumentation for minority class
# class_counts = df['Sentiment'].value_counts()
# minority_class = class_counts.idxmin()


# Iterating once through the data and augment each row the same number of times
for text, label in zip(df["Feedback"], df["Sentiment"]):
    for _ in range(N_AUG):
        try:
            augmented_text = aug.augment(text)
            augmented_rows.append({"Feedback": augmented_text, "Sentiment": label})
        except Exception as e:
            print(f"Augmentation error on text: {text} | Error: {e}")

# Creating augmented DataFrame
df_augmented = pd.DataFrame(augmented_rows)

# Combining original + augmented data
df_nlpaug = pd.concat([df, df_augmented], ignore_index=True)

print(f"Original size: {len(df)}")
print(f"Augmented size: {len(df_augmented)}")
print(f"Combined size: {len(df_nlpaug)}")

#augmented examples
print(df_nlpaug.sample(5))

# Remove the 'tokens' columns from the combined dataframe
df_nlpaug = df_nlpaug.drop(columns=['tokens'], errors='ignore')
df_nlpaug.info() #chck for dtype and any null values

df_nlpaug['Sentiment'].value_counts() #check if augumented data is balanced

df_nlpaug.to_csv('NLPAug_augmented_feedback_data.csv', index=False) #data df_nlpaug to csv

"""#### LLM AUG"""

import os, getpass
openai_api_key = os.getenv('OPENAI_API_KEY') or getpass.getpass('OpenAI API key: ')
os.environ['OPENAI_API_KEY'] = ''
#inserted my openai api key
#attached the generated file in supporting documents for running this analysis
import openai
openai.api_key = openai_api_key

from typing import List

def generate_variations(text: str, label: str, n: int = 3, model: str = 'gpt-4o') -> List[str]:
    """Generate *n* paraphrases that keep the original sentiment label."""
    prompt = (
        f"You are an NLP assistant generating diverse paraphrases for data augmentation.\n"
        f"Your task is to produce {n} different English sentences that have the same overall sentiment ({label}) as the given sentence.\n"
        f"Original sentence:\n\"{text}\"\n"
        f"Return each new sentence on a separate line without numbering."
    )

    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",#using gpt-4o-mini model
        temperature=0.85,
        max_tokens=120,
        messages=[{"role": "user", "content": prompt}],
    )

    sentences = [s.strip() for s in resp.choices[0].message.content.split('\n') if s.strip()]
    return sentences[:n] #strip each sentence and return

def augment_dataset(df: pd.DataFrame, n_aug: int = 3, balance: bool = False) -> pd.DataFrame:
    augmented_rows = []
    # Determine target size per class if balancing
    if balance:
        class_counts = df['Sentiment'].value_counts()
        target_size = class_counts.max()
    else:
        target_size = None

    for _, row in df.iterrows():
        text, label = row['Feedback'], row['Sentiment']
        augmented_rows.append(row.to_dict())  # keep original

        # Decide how many augments for this sample
        if balance:
            label_count = (df['Sentiment'] == label).sum()
            this_n = max(n_aug, (target_size - label_count) // label_count if label_count else n_aug)
        else:
            this_n = n_aug

        try:
            new_sents = generate_variations(text, label, this_n)
            for s in new_sents:
                augmented_rows.append({"Feedback": s, "Sentiment": label})
        except Exception as e:
            print(f"[WARN] Generation failed for: {text[:30]}... -> {e}")
            continue

    return pd.DataFrame(augmented_rows)

n_aug = 5    # number of new examples per original #n=5 to balance nlpaug and comparing results
balance = True # ensuring class balancing

df_llm = augment_dataset(df, n_aug=n_aug, balance=balance)
print(f"Augmented dataset size: {len(df_llm)} (original {len(df)})")
display(df_llm.head())
print('\nNew label distribution:')
display(df_llm['Sentiment'].value_counts())



# remove column tokens in df_llm

df_llm = df_llm.drop(columns=['tokens'], errors='ignore')

df_llm.head()

df_llm.to_csv('llm_augmented_feedback_data.csv', index=False) #please find data attached as a supporting document for further analaysis

"""## Data Preprocessing


"""

import nltk
from nltk.corpus import stopwords
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
STOP_WORDS = set(stopwords.words("english"))

import nltk
nltk.download('punkt_tab')

"""### Duplication"""

def drop_exact_duplicates(df: pd.DataFrame, text_col: str) -> pd.DataFrame:
    #Dropping duplicate row, case insensitive
    return df.drop_duplicates(subset=text_col, keep="first")

"""### Sensitive Information Removal

"""

EMAIL_RE = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b")
PHONE_RE = re.compile(r"\b(?:\+?\d{1,3}[-.\s])?(?:\(?\d{3}\)?[-.\s]?){1,2}\d{4}\b")
URL_RE   = re.compile(r"https?://\S+|www\.\S+")

def scrub_sensitive(text: str) -> str:
    #Masking  e‑mails, phone numbers, and URLs with placeholders
    text = EMAIL_RE.sub("[EMAIL]", text)
    text = PHONE_RE.sub("[PHONE]", text)
    text = URL_RE.sub("[URL]", text)
    return text

"""###Spam Filtering"""

def is_spam(text: str,
            min_alpha_ratio: float = 0.3,
            min_len: int = 15,
            max_rep: int = 6) -> bool:
    #flagging spams: very short text, few characters, repeated characters

    if len(text) < min_len:
        return True

    alpha_ratio = sum(ch.isalpha() for ch in text) / len(text)
    if alpha_ratio < min_alpha_ratio:
        return True

    if re.search(rf"(.)\1{{{max_rep},}}", text):
        return True

    return False

"""### Normalisation"""

PUNCT_DIGIT_RE = re.compile(r"[^A-Za-z\s]")   # keeping letters & space

def normalise(text: str) -> str:
    """
    • Unicode NFKC normalisation
    • Lower‑case
    • Strip accents
    • Remove punctuation & digits
    • Collapse multiple spaces
    """
    text = unicodedata.normalize("NFKC", text)
    text = text.lower()
    text = unicodedata.normalize(
        "NFKD", text
    ).encode("ascii", "ignore").decode("utf-8", "ignore")
    text = PUNCT_DIGIT_RE.sub(" ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

"""### Tokenization"""

from nltk.tokenize import sent_tokenize,word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
def tokenize(text: str) -> List[str]:
    """Word‑tokenise & drop stop‑words."""
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in STOP_WORDS]
    return tokens

"""###Sentiment lable mapping"""

LABEL_MAP = {"positive": 1, "negative": 0}

def map_sentiment(label) -> int:
# label mapping, case insensitive
    try:
        return LABEL_MAP[str(label).strip().lower()]
    except KeyError as e:
        raise ValueError(f"Unknown sentiment label: {label}") from e

"""## Preprocessing pipeline"""

import unicodedata
import re # Assuming re is used in scrub_sensitive or is_spam
def preprocess_dataframe(
    df: pd.DataFrame,
    text_col: str = "Feedback",
    label_col: str = "Sentiment",
) -> pd.DataFrame:

    # Exact‑text deduplication
    df = drop_exact_duplicates(df, text_col)

    # Sensitive‑info scrub + spam filter (row‑wise)
    cleaned_records = []
    for _, row in df.iterrows():
        text = scrub_sensitive(str(row[text_col]))

        if is_spam(text):
            continue  # dropping spam row

        text_norm = normalise(text)
        tokens = tokenize(text_norm)

        # Skippking empty token lists (after stop‑word removal)
        if not tokens:
            continue

        cleaned_records.append(
            {
                text_col: " ".join(tokens),     # tokenised text re‑joined
                f"{text_col}_tokens": tokens,
                label_col: map_sentiment(row[label_col]),
            }
        )

    return pd.DataFrame(cleaned_records)

if __name__ == "__main__":
    processed = preprocess_dataframe(df_nlpaug)
    print(processed.head())

"""#### df_nlp_preprocessed"""

df_nlpaug_clean = preprocess_dataframe(df_nlpaug,
                                text_col="Feedback",
                                label_col="Sentiment")

print(df_nlpaug_clean.describe(include="all"))

"""####df_llm_preprocessed"""

df_llm_clean = preprocess_dataframe(df_llm,
                                text_col="Feedback",
                                label_col="Sentiment")

print(df_llm_clean.describe(include="all"))

df_nlpaug_clean= df_nlpaug_clean.drop(columns=['Feedback_tokens'], errors='ignore')

df_nlpaug_clean

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df_nlpaug_clean)

df_llm_clean= df_llm_clean.drop(columns=['Feedback_tokens'], errors='ignore')

df_llm_clean

"""#APPENDIX [please use own Wandb Authentication code for running]

### NLPAUG data training and Evaluation
"""

import pandas as pd, torch, numpy as np
from torch.utils.data import Dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          Trainer, TrainingArguments)
from sklearn.model_selection import train_test_split

df_nlpaug_clean = df_nlpaug_clean.dropna(subset=['Feedback', 'Sentiment'])          # safeguard

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df_nlpaug_clean['Feedback'].tolist(),
    df_nlpaug_clean['Sentiment'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_nlpaug_clean['Sentiment']
)

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(texts, truncation=True, padding=True)
        self.labels    = labels
    def __len__(self):  return len(self.labels)
    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


#  Train each model and store trainer / validation data

model_names    = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']
trainers_dict  = {}
valsets_dict   = {}
results_dict   = {}

def compute_metrics(eval_pred):
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    p,r,f,_ = precision_recall_fscore_support(labels, preds, average='binary')
    acc     = accuracy_score(labels, preds)
    return {'accuracy':acc,'precision':p,'recall':r,'f1':f}

for model_name in model_names:
    print(f'\n Training {model_name}')
    tokenizer   = AutoTokenizer.from_pretrained(model_name)
    model       = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    # Same split for all models
    val_texts, val_labels = test_texts, test_labels   # rename for clarity
    train_ds  = SentimentDataset(train_texts, train_labels, tokenizer)
    val_ds    = SentimentDataset(val_texts,   val_labels,   tokenizer)

    args = TrainingArguments(
        output_dir=f'./results/{model_name.replace("/","_")}',
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,

        logging_dir='./logs',
        seed=42
    )
    trainer = Trainer(model=model,
                      args=args,
                      train_dataset=train_ds,
                      eval_dataset=val_ds,
                      compute_metrics=compute_metrics)

    trainer.train()
    res = trainer.evaluate()
    print(res)

    # save for later evaluation
    trainers_dict[model_name] = trainer
    valsets_dict[model_name]  = val_ds
    results_dict[model_name]  = res

import matplotlib.pyplot as plt, seaborn as sns, numpy as np
from scipy.special import softmax
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_curve, auc)

def evaluate_one(trainer, dataset, y_true):
    logits = trainer.predict(dataset).predictions
    y_pred = logits.argmax(-1)
    y_prob = softmax(logits, axis=1)[:,1]          # P(Positive)

    report = classification_report(y_true, y_pred,
                                   target_names=['Negative','Positive'],
                                   output_dict=True)
    cm     = confusion_matrix(y_true, y_pred)
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val     = auc(fpr, tpr)
    return {'y_pred':y_pred,'y_prob':y_prob,'report':report,
            'cm':cm,'fpr':fpr,'tpr':tpr,'auc':auc_val}

########################################################################
#  Run evaluation for NLP AUG DATASET
########################################################################
all_metrics = []
for name, trainer in trainers_dict.items():
    print(f'  Evaluating {name}')
    y_true  = np.array(test_labels)        # same ground‑truth for all
    metric  = evaluate_one(trainer, valsets_dict[name], y_true)
    metric['name'] = name
    all_metrics.append(metric)

########################################################################
#  1) Classification reports
########################################################################
for m in all_metrics:
    print(f'\n===== {m["name"]} =====')
    print(classification_report(test_labels, m['y_pred'],
                                target_names=['Negative','Positive']))

########################################################################
#  2) Confusion‑matrix heat‑maps
########################################################################
fig, axes = plt.subplots(1, len(all_metrics), figsize=(5*len(all_metrics),4))
if len(all_metrics)==1: axes=[axes]
for ax, m in zip(axes, all_metrics):
    sns.heatmap(m['cm'], annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
    ax.set_title(m['name']); ax.set_xlabel('Predicted'); ax.set_ylabel('True')
plt.tight_layout(); plt.show()

########################################################################
#  3) ROC curves
########################################################################
plt.figure(figsize=(6,5))
for m in all_metrics:
    plt.plot(m['fpr'], m['tpr'], label=f"{m['name']} (AUC={m['auc']:.3f})")
plt.plot([0,1],[0,1],'k--',lw=1)
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC curves on test set'); plt.legend(); plt.grid(True); plt.tight_layout()
plt.show()

"""###LLM data training and *Evaluation*"""

import pandas as pd, torch, numpy as np
from torch.utils.data import Dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          Trainer, TrainingArguments)
from sklearn.model_selection import train_test_split

df_llm_clean = df_llm_clean.dropna(subset=['Feedback', 'Sentiment'])          # safeguard

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df_llm_clean['Feedback'].tolist(),
    df_llm_clean['Sentiment'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_llm_clean['Sentiment']
)

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(texts, truncation=True, padding=True)
        self.labels    = labels
    def __len__(self):  return len(self.labels)
    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


#  Train each model and store trainer / validation data

model_names    = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']
trainers_dict  = {}
valsets_dict   = {}
results_dict   = {}

def compute_metrics(eval_pred):
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    p,r,f,_ = precision_recall_fscore_support(labels, preds, average='binary')
    acc     = accuracy_score(labels, preds)
    return {'accuracy':acc,'precision':p,'recall':r,'f1':f}

for model_name in model_names:
    print(f'\n Training {model_name}')
    tokenizer   = AutoTokenizer.from_pretrained(model_name)
    model       = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    # Same split for all models
    val_texts, val_labels = test_texts, test_labels   # rename for clarity
    train_ds  = SentimentDataset(train_texts, train_labels, tokenizer)
    val_ds    = SentimentDataset(val_texts,   val_labels,   tokenizer)

    args = TrainingArguments(
        output_dir=f'./results/{model_name.replace("/","_")}',
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,

        logging_dir='./logs',
        seed=42
    )
    trainer = Trainer(model=model,
                      args=args,
                      train_dataset=train_ds,
                      eval_dataset=val_ds,
                      compute_metrics=compute_metrics)

    trainer.train()
    res = trainer.evaluate()
    print(res)

    # save for later evaluation
    trainers_dict[model_name] = trainer
    valsets_dict[model_name]  = val_ds
    results_dict[model_name]  = res

import matplotlib.pyplot as plt, seaborn as sns, numpy as np
from scipy.special import softmax
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_curve, auc)

def evaluate_one(trainer, dataset, y_true):
    logits = trainer.predict(dataset).predictions
    y_pred = logits.argmax(-1)
    y_prob = softmax(logits, axis=1)[:,1]          # P(Positive)

    report = classification_report(y_true, y_pred,
                                   target_names=['Negative','Positive'],
                                   output_dict=True)
    cm     = confusion_matrix(y_true, y_pred)
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val     = auc(fpr, tpr)
    return {'y_pred':y_pred,'y_prob':y_prob,'report':report,
            'cm':cm,'fpr':fpr,'tpr':tpr,'auc':auc_val}

########################################################################
#  Run evaluation for NLP AUG DATASET
########################################################################
all_metrics = []
for name, trainer in trainers_dict.items():
    print(f'  Evaluating {name}')
    y_true  = np.array(test_labels)        # same ground‑truth for all
    metric  = evaluate_one(trainer, valsets_dict[name], y_true)
    metric['name'] = name
    all_metrics.append(metric)

########################################################################
#  1) Classification reports
########################################################################
for m in all_metrics:
    print(f'\n===== {m["name"]} =====')
    print(classification_report(test_labels, m['y_pred'],
                                target_names=['Negative','Positive']))

########################################################################
#  2) Confusion‑matrix heat‑maps
########################################################################
fig, axes = plt.subplots(1, len(all_metrics), figsize=(5*len(all_metrics),4))
if len(all_metrics)==1: axes=[axes]
for ax, m in zip(axes, all_metrics):
    sns.heatmap(m['cm'], annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
    ax.set_title(m['name']); ax.set_xlabel('Predicted'); ax.set_ylabel('True')
plt.tight_layout(); plt.show()

########################################################################
#  3) ROC curves
########################################################################
plt.figure(figsize=(6,5))
for m in all_metrics:
    plt.plot(m['fpr'], m['tpr'], label=f"{m['name']} (AUC={m['auc']:.3f})")
plt.plot([0,1],[0,1],'k--',lw=1)
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC curves on test set'); plt.legend(); plt.grid(True); plt.tight_layout()
plt.show()